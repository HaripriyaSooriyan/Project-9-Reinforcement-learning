{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8eyWgS6H4p_"
   },
   "source": [
    "<h1><center>Module 10 - Reinforcement Learning</center>\n",
    "    Case Study 2: Frozen Lake Environment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P31HDAZYLGLm"
   },
   "source": [
    "Goal of this project:\n",
    "<li>Teach an AI how to solve the Frozen Lake environment using reinforcement learning. \n",
    "<li>Let us use a pre-existing simulation environment like OpenAI Gym and non-slippery version to begin with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uu-EwFV5JGjw"
   },
   "source": [
    "<b>Frozen Lake environment </b> \n",
    "In reinforcement learning, this is a problem where the agent navigates a grid of icy terrain. \n",
    "The Frozen Lake environment is a 4×4 grid which contain four possible areas - Safe (S), Frozen (F), Hole that gets you stuck forever(H) and Goal (G). The AI,or agent moves around the grid until it reaches the goal or the hole. If it falls into the hole, it has to start from the beginning and is rewarded the value 0. The process continues until it learns from every mistake and reaches the goal eventually. The AI, or agent, has 4 possible actions: go LEFT, DOWN, RIGHT, or UP. The agent must learn to avoid holes in order to reach the goal in a minimal number of actions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqbiByOjKyc3"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaBhgYDn8xcJ"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XU82_Br4-3jN"
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1',new_step_api=True, is_slippery=False)   #use frozen lake environment from gym library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmWdb-6IO44o"
   },
   "source": [
    "Our agent can be found in 16 different positions, called <b>states</b>. For each state, there are 4 possible <b> actions</b>: LEFT, DOWN, RIGHT, or UP. Learning how to play Frozen Lake is like learning which action you should choose in every state. To know which action is the best in a given state, we would like to assign a quality value to our actions. We have 16 states and 4 actions, so have to calculate 16×4 = 64 values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiQLhYLUQLsl"
   },
   "source": [
    "The values are represented using a table, known as a Q-table, where rows list every state (s) and columns list every action (a). In this Q-table, each cell contains a value <b>Q(s,a)</b>, which is the value (quality) of the action in the state. (1 if it's the best action possible, 0 if it's really bad). When our agent is in a particular state, it just has to check this table to see which action has the highest value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XdeVK02_C_8",
    "outputId": "84d8d78a-417d-42c2-f7ca-af07fe8d33d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "4\n"
     ]
    }
   ],
   "source": [
    " \n",
    "state = env.observation_space.n   # get the number of states\n",
    "action = env.action_space.n       # get the number of actions\n",
    "print(state)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k0JctXwoMgIJ",
    "outputId": "46a81a05-86e1-4a69-8432-af23694c65bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()      # reset the environment to default state\n",
    "# env.render()   # render the GUI for the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tV939OgQw1G"
   },
   "source": [
    "Let's create a Q-table and fill it with zeros since we still have no idea of the value of each action in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VL4w-QGsIlUa",
    "outputId": "ef5b0a6c-ef83-44e9-dfa9-2e905258df23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((state,action))\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3hja2uyIlf6"
   },
   "outputs": [],
   "source": [
    "episodes = 500\n",
    "max_steps = 100\n",
    "learning_rate = 0.5\n",
    "gamma = 0.9\n",
    "render = False\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PW_zOoYR1Z5_",
    "outputId": "989ccdf7-1e8d-47d4-940e-f362dac63130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table after training:\n",
      "[[0.531441   0.59049    0.59049    0.531441  ]\n",
      " [0.531441   0.         0.6561     0.59049   ]\n",
      " [0.59049    0.729      0.59049    0.6561    ]\n",
      " [0.6561     0.         0.59048353 0.59048998]\n",
      " [0.59048999 0.6561     0.         0.531441  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.81       0.         0.6561    ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.65609999 0.         0.729      0.59049   ]\n",
      " [0.6561     0.80999994 0.81       0.        ]\n",
      " [0.729      0.9        0.         0.729     ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.80896108 0.9        0.72893905]\n",
      " [0.81       0.9        1.         0.81      ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "outcomes = []\n",
    "\n",
    "# Training\n",
    "for i in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # By default, we consider our outcome to be a failure\n",
    "    outcomes.append(\"Failure\")\n",
    "    \n",
    "    # Until the agent gets stuck in a hole or reaches the goal, keep training it\n",
    "    while not done:\n",
    "        # Generate a random number between 0 and 1\n",
    "        rnd = np.random.random()\n",
    "\n",
    "        # If random number < epsilon, take a random action\n",
    "        if rnd < epsilon:\n",
    "          action = env.action_space.sample()\n",
    "        # Else, take the action with the highest value in the current state\n",
    "        else:\n",
    "          action = np.argmax(Q[state])\n",
    "             \n",
    "        # Implement this action and move the agent in the desired direction\n",
    "        new_state, reward, done, info, extra = env.step(action)\n",
    "        \n",
    "        # Update Q(s,a)\n",
    "        Q[state, action] = Q[state, action] + learning_rate * (reward + gamma * np.max(Q[new_state]) - Q[state, action])\n",
    "                                \n",
    "        # Update our current state\n",
    "        state = new_state\n",
    "\n",
    "        # If we have a reward, it means that our outcome is a success\n",
    "        if reward:\n",
    "          outcomes[-1] = \"Success\"\n",
    "\n",
    "    # Update epsilon\n",
    "    epsilon = max(epsilon - epsilon_decay, 0)\n",
    "\n",
    "print('Q-table after training:')\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UXsXRqnAlLm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
